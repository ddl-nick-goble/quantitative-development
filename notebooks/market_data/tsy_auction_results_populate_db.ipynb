{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7a2c534-c854-4afa-8d8b-f2d0996485be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data_env' not found in environment. Defaulting to 'sandbox' env.\n",
      "setting env to sandbox data\n",
      "getting data source for sandbox\n",
      "year 2024\n",
      "  2024: fetched 10000 rows\n",
      "year 2025\n",
      "  2025: fetched 10000 rows\n",
      "Upserting 10000 rows in batches of 1000...\n",
      "  Batch 1: 1000 rows\n",
      "  Batch 2: 1000 rows\n",
      "  Batch 3: 1000 rows\n",
      "  Batch 4: 1000 rows\n",
      "  Batch 5: 1000 rows\n",
      "  Batch 6: 1000 rows\n",
      "  Batch 7: 1000 rows\n",
      "  Batch 8: 1000 rows\n",
      "  Batch 9: 1000 rows\n",
      "  Batch 10: 1000 rows\n",
      "✅ Loaded 10000 rows in 40.3s\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import data.data_source as data_source\n",
    "from utils.artifact_saver import get_artifact_path\n",
    "\n",
    "API_BASE   = \"https://api.fiscaldata.treasury.gov/services/api/fiscal_service/v1/accounting/od/auctions_query\"\n",
    "PAGE_SIZE  = 10000   # plenty for a single year\n",
    "BATCH_SIZE = 1000     # rows per upsert batch\n",
    "\n",
    "ds = data_source.get_data_source()\n",
    "\n",
    "def fetch_all_auctions_last_y(years_to_backfill) -> list[dict]:\n",
    "    rows = []\n",
    "    start_year = (date.today() - relativedelta(years=years_to_backfill)).year\n",
    "    end_year   = date.today().year\n",
    "\n",
    "    for yr in range(start_year, end_year + 1):\n",
    "        print('year', yr)\n",
    "        start = date(yr, 1, 1).isoformat()\n",
    "        end   = date(yr, 12, 31).isoformat()\n",
    "        params = [\n",
    "            (\"filter\", f\"record_date:gte:{start}\"),\n",
    "            (\"filter\", f\"record_date:lte:{end}\"),\n",
    "            (\"page[size]\", str(PAGE_SIZE)),\n",
    "            (\"sort\", \"record_date\"),\n",
    "        ]\n",
    "        resp = requests.get(API_BASE, params=params)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json().get(\"data\", [])\n",
    "        print(f\"  {yr}: fetched {len(data)} rows\")\n",
    "        rows.extend(data)\n",
    "    return rows\n",
    "\n",
    "def quote_value(val):\n",
    "    if val is None or val == \"null\":\n",
    "        return \"NULL\"\n",
    "    try:\n",
    "        float(val)\n",
    "        return str(val)\n",
    "    except:\n",
    "        return \"'\" + str(val).replace(\"'\", \"''\") + \"'\"\n",
    "\n",
    "def chunks(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i : i + n]\n",
    "\n",
    "def dedupe(records: list[dict]) -> list[dict]:\n",
    "    seen = set()\n",
    "    deduped = []\n",
    "    for r in records:\n",
    "        key = (r.get(\"record_date\"), r.get(\"cusip\"))\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        deduped.append(r)\n",
    "    return deduped\n",
    "\n",
    "def upsert_batch(records: list[dict], batch_size: int = BATCH_SIZE):\n",
    "    if not records:\n",
    "        return\n",
    "    cols = list(records[0].keys())\n",
    "    col_list = \", \".join(cols)\n",
    "    total = len(records)\n",
    "    print(f\"Upserting {total} rows in batches of {batch_size}...\")\n",
    "    for idx, chunk in enumerate(chunks(records, batch_size), start=1):\n",
    "        print(f\"  Batch {idx}: {len(chunk)} rows\")\n",
    "        values_sql = []\n",
    "        for r in chunk:\n",
    "            vals = [quote_value(r.get(c)) for c in cols]\n",
    "            values_sql.append(\"(\" + \", \".join(vals) + \")\")\n",
    "        values_str = \",\\n\".join(values_sql)\n",
    "        set_list = \", \".join([f\"{c} = EXCLUDED.{c}\" for c in cols])\n",
    "        sql = f\"\"\"\n",
    "        INSERT INTO tsy_auction_results ({col_list})\n",
    "        VALUES\n",
    "        {values_str}\n",
    "        ON CONFLICT (record_date, cusip)\n",
    "        DO UPDATE SET\n",
    "          {set_list};\n",
    "        \"\"\"\n",
    "        ds.query(sql)\n",
    "\n",
    "def main(years_to_backfill):\n",
    "    t0 = time.time()\n",
    "\n",
    "    data = fetch_all_auctions_last_y(years_to_backfill)\n",
    "    data = dedupe(data)\n",
    "    upsert_batch(data)\n",
    "\n",
    "    duration = time.time() - t0\n",
    "    print(f\"✅ Loaded {len(data)} rows in {duration:.1f}s\")\n",
    "\n",
    "    def find_mount_root(start: Path, target: str = \"mnt\") -> Path:\n",
    "        \"\"\"Climb up until we find the given folder name.\"\"\"\n",
    "        current = start.resolve()\n",
    "        while current.name != target:\n",
    "            if current.parent == current:\n",
    "                raise FileNotFoundError(f\"Could not find folder named '{target}' in parent paths.\")\n",
    "            current = current.parent\n",
    "        return current\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    fn = get_artifact_path(\"tsy_auction_results.csv\")\n",
    "    df.to_csv(fn, index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(years_to_backfill = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309d5914-6d2e-403b-8812-567f4d08b4d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
