{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4753bf2-0c13-492f-afae-f28616da8db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data_env' not found in environment. Defaulting to 'sandbox' env.\n",
      "setting env to sandbox data\n",
      "2025: prepared 98 rows\n",
      "getting data source for sandbox\n",
      "✅ Done bulk-loading rate_curves from 2025-05-31 through 2025-06-10\n",
      "🏃 View run abundant-mouse-996 at: http://127.0.0.1:8768/#/experiments/1500/runs/0083c4ae5ab74db3a46c554799efc1cb\n",
      "🧪 View experiment at: http://127.0.0.1:8768/#/experiments/1500\n"
     ]
    }
   ],
   "source": [
    "import data.data_source as data_source\n",
    "import time\n",
    "from domino.data_sources import DataSourceClient\n",
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from functools import lru_cache\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from config import env\n",
    "from utils.artifact_saver import get_artifact_path\n",
    "\n",
    "import mlflow\n",
    "import os\n",
    "\n",
    "experiment_name = f\"Populate Tsy Curve [{env}]\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# ─── Helpers ────────────────────────────────────────────────────────────────\n",
    "\n",
    "@lru_cache(maxsize=100)\n",
    "def fetch_treasury_csv(year: int) -> str:\n",
    "    url = (\n",
    "        f\"https://home.treasury.gov/resource-center/data-chart-center/interest-rates/\"\n",
    "        f\"daily-treasury-rates.csv/{year}/all\"\n",
    "        f\"?field_tdr_date_value={year}\"\n",
    "        f\"&type=daily_treasury_yield_curve&page&_format=csv\"\n",
    "    )\n",
    "    resp = requests.get(url)\n",
    "    resp.raise_for_status()\n",
    "    return resp.text\n",
    "\n",
    "def parse_tenor(tenor_str: str) -> float:\n",
    "    num_str, unit = tenor_str.strip().split(maxsplit=1)\n",
    "    n = float(num_str); u = unit.lower()\n",
    "    if u.startswith('mo'):   return (n * 30) / 360\n",
    "    if u.startswith('yr'):   return n\n",
    "    if u.startswith('day'):  return n / 360\n",
    "    raise ValueError(f\"Unknown tenor unit: '{unit}'\")\n",
    "\n",
    "def prepare_year_rows(year, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fetch & parse a single year's CSV, return a flat list of rows:\n",
    "    (curve_type, date, tenor, rate, tenor_num)\n",
    "    \"\"\"\n",
    "    text = fetch_treasury_csv(year)\n",
    "    df   = pd.read_csv(StringIO(text), parse_dates=['Date'], index_col='Date')\n",
    "    df = df[(df.index.date >= start_date) & (df.index.date <= end_date)]\n",
    "    rows = []\n",
    "    for ts, row in df.iterrows():\n",
    "        d = ts.date()\n",
    "        for tenor, rate in row.items():\n",
    "            if pd.isna(rate):\n",
    "                continue\n",
    "            rows.append((\n",
    "                'US Treasury Par',\n",
    "                d.isoformat(),\n",
    "                tenor,\n",
    "                float(rate),\n",
    "                parse_tenor(tenor)\n",
    "            ))\n",
    "    return year, rows\n",
    "\n",
    "# ─── Main loader ────────────────────────────────────────────────────────────\n",
    "\n",
    "\n",
    "def populate(\n",
    "    days: int,\n",
    "    batch_size: int    = 5000,\n",
    "    fetch_workers: int = 4,\n",
    "    write_workers: int = 2\n",
    "):\n",
    "    \n",
    "    \"\"\"\n",
    "    Populate rate_curves for the last `days` days (up to today),\n",
    "    but not before 2010-03-15.\n",
    "    \"\"\"\n",
    "    # calculate date range\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_param(\"days_requested\", days)\n",
    "        mlflow.log_param(\"starting_domino_user\", os.environ[\"DOMINO_STARTING_USERNAME\"])\n",
    "        mlflow.log_param(\"batch_size\", batch_size)\n",
    "        mlflow.log_param(\"fetch_workers\", fetch_workers)\n",
    "        mlflow.log_param(\"write_workers\", write_workers)\n",
    "\n",
    "        start_time = time.time()\n",
    "        end_date = date.today()\n",
    "        start_date = end_date - relativedelta(days=days)\n",
    "        min_date = date(2010, 3, 15)\n",
    "        unique_dates = set()\n",
    "        if start_date < min_date:\n",
    "            start_date = min_date\n",
    "    \n",
    "        years = list(range(start_date.year, end_date.year + 1))\n",
    "    \n",
    "        # 1) parallel fetch + parse per-year\n",
    "        rows_by_year = {}\n",
    "        with ThreadPoolExecutor(max_workers=fetch_workers) as fetch_pool:\n",
    "            futures = {\n",
    "                fetch_pool.submit(prepare_year_rows, y, start_date, end_date): y\n",
    "                for y in years\n",
    "            }\n",
    "            for fut in as_completed(futures):\n",
    "                y = futures[fut]\n",
    "                try:\n",
    "                    year, rows = fut.result()\n",
    "                    if rows:\n",
    "                        rows_by_year[year] = rows\n",
    "                        print(f\"{year}: prepared {len(rows)} rows\")\n",
    "                    else:\n",
    "                        print(f\"{year}: no data → skipped\")\n",
    "                except Exception as e:\n",
    "                    print(f\"{y}: error fetching/parsing → {e}\")\n",
    "\n",
    "        ds = data_source.get_data_source()\n",
    "\n",
    "        # 2) for each year, batch & fire INSERTs in parallel\n",
    "        def write_batch(batch):\n",
    "            unique_dates.update([r[1] for r in batch])\n",
    "            vals = \", \".join(\n",
    "                f\"('{r[0]}','{r[1]}','{r[2]}',{r[3]},{r[4]})\"\n",
    "                for r in batch\n",
    "            )\n",
    "            sql = f\"\"\"\n",
    "                INSERT INTO rate_curves\n",
    "                  (curve_type, curve_date, tenor_str, rate, tenor_num)\n",
    "                VALUES\n",
    "                  {vals}\n",
    "                ON CONFLICT (curve_type, curve_date, tenor_str) DO UPDATE\n",
    "                  SET rate      = EXCLUDED.rate,\n",
    "                      tenor_num = EXCLUDED.tenor_num;\n",
    "                \"\"\"\n",
    "            ds.query(sql)\n",
    "            \n",
    "        with ThreadPoolExecutor(max_workers=write_workers) as write_pool:\n",
    "            write_futures = []\n",
    "            for year, rows in rows_by_year.items():\n",
    "                for i in range(0, len(rows), batch_size):\n",
    "                    batch = rows[i : i + batch_size]\n",
    "                    write_futures.append(write_pool.submit(write_batch, batch))\n",
    "    \n",
    "            for fut in as_completed(write_futures):\n",
    "                try:\n",
    "                    fut.result()\n",
    "                except Exception as e:\n",
    "                    print(f\"Write error: {e}\")\n",
    "\n",
    "\n",
    "        duration = time.time() - start_time\n",
    "        num_rows  = sum(len(r) for r in rows_by_year.values())\n",
    "\n",
    "        # log metrics\n",
    "        mlflow.log_metric(\"days_loaded\", len(unique_dates))\n",
    "        mlflow.log_metric(\"rows_loaded\", num_rows)\n",
    "        mlflow.log_metric(\"duration_seconds\", duration)\n",
    "\n",
    "        # artifact: snapshot all rows as CSV\n",
    "        all_rows = [r for rows in rows_by_year.values() for r in rows]\n",
    "        df_all = pd.DataFrame(all_rows, columns=[\n",
    "            \"curve_type\", \"curve_date\", \"tenor_str\", \"rate\", \"tenor_num\"\n",
    "        ])\n",
    "        csv_path = get_artifact_path(\"rate_curves_loaded.csv\")\n",
    "        df_all.to_csv(csv_path, index=False)\n",
    "        mlflow.log_artifact(csv_path, artifact_path=\"rate_curves\")\n",
    "        \n",
    "        print(\"✅ Done bulk-loading rate_curves \"\n",
    "              f\"from {start_date} through {end_date}\")\n",
    "\n",
    "# ─── MAIN ───────────────────────────────────────────────────────────────────\n",
    "# arg1 is the number of days to backdate.\n",
    "# 1 => yesterday's curve, 100 => last 100 days.\n",
    "default_backdated_days = 10\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    d = default_backdated_days\n",
    "else:\n",
    "    try:\n",
    "        days_to_backdate = sys.argv[1]\n",
    "        d = int(days_to_backdate)\n",
    "    except Exception as e:\n",
    "        d = default_backdated_days\n",
    "\n",
    "populate(days=d)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ba97e5-96ae-4277-af4b-8ac622368a1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
