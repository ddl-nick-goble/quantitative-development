{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a888706f-5367-4bf6-af03-a31fc2278e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting data source for sandbox\n",
      "# 1) BULK FETCH HISTORY\n",
      "# 2) BULK FETCH CONES\n",
      "# 3) BULK FETCH REALIZED RATES + FORWARD-FILL\n",
      "# 4) COMPUTE & ASSEMBLE ROWS\n",
      "# 5) BATCH UPSERT\n",
      "🏃 View run aged-pug-709 at: http://127.0.0.1:8768/#/experiments/1524/runs/91eef4f5cbe044598bb042b4386732c9\n",
      "🧪 View experiment at: http://127.0.0.1:8768/#/experiments/1524\n",
      "✅ Done: upserted 35555 rows into rate_cone_diagnostics.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import sys\n",
    "from datetime import date, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "\n",
    "import data.data_source as data_source\n",
    "from config import env\n",
    "from utils.artifact_saver import get_artifact_path\n",
    "\n",
    "# ─── CONFIG ─────────────────────────────────────────────────────────────────\n",
    "\n",
    "mlflow.set_experiment(f\"Populate IR Cone Metrics [{env}]\")\n",
    "\n",
    "CURVE_TYPE     = \"US Treasury Par\"\n",
    "TENORS         = [1/12, 0.125, 2/12, 0.25, 4/12, 0.5, 1, 2, 3, 5, 7, 10, 20, 30]\n",
    "FIT_YEARS      = 1\n",
    "BACKDATE_DAYS  = 365\n",
    "BATCH_SIZE     = 500\n",
    "\n",
    "ds = data_source.get_data_source()\n",
    "\n",
    "# ─── COLUMN LISTS ───────────────────────────────────────────────────────────\n",
    "\n",
    "COLUMNS = [\n",
    "  \"curve_type\",\"model_type\",\"curve_date\",\"days_forward\",\"tenor_num\",\"tenor_str\",\n",
    "  \"forecast_p01\",\"forecast_p05\",\"forecast_p10\",\"forecast_p50\",\n",
    "  \"forecast_p90\",\"forecast_p95\",\"forecast_p99\",\n",
    "  \"realized_date\",\"realized_rate\",\n",
    "  \"forecast_error\",\"absolute_error\",\"relative_error\",\n",
    "  \"percentile_rank\",\"inside_cone\",\n",
    "  \"n_obs_fit\",\"total_variance\",\"trace_covariance\"\n",
    "]\n",
    "UPSERT_COLS = COLUMNS[5:]  # everything after tenor_str\n",
    "\n",
    "# ─── UTILITY ─────────────────────────────────────────────────────────────────\n",
    "\n",
    "def percentile_rank(real: float, pct_map: dict[int,float]) -> float:\n",
    "    xs, ys = zip(*sorted(pct_map.items()))\n",
    "    return float(np.interp(real, ys, xs))\n",
    "\n",
    "# ─── POPULATOR ───────────────────────────────────────────────────────────────\n",
    "\n",
    "def populate(days: int) -> list[tuple]:\n",
    "    today     = date.today()\n",
    "    end_date  = today - timedelta(days=1)\n",
    "    asof_dates = [end_date - timedelta(days=d) for d in range(1, days+1)]\n",
    "    min_asof   = min(asof_dates)\n",
    "    max_asof   = max(asof_dates)\n",
    "\n",
    "    print('# 1) BULK FETCH HISTORY')\n",
    "    hist_start = max(min_asof - relativedelta(years=FIT_YEARS), date(2010,1,1))\n",
    "    tstr       = \",\".join(map(str, TENORS))\n",
    "    hist_q = f\"\"\"\n",
    "      SELECT curve_date, tenor_num, rate\n",
    "        FROM rate_curves\n",
    "       WHERE curve_type = '{CURVE_TYPE}'\n",
    "         AND curve_date BETWEEN '{hist_start}' AND '{end_date}'\n",
    "         AND tenor_num IN ({tstr})\n",
    "    \"\"\"\n",
    "    hist_df = ds.query(hist_q).to_pandas()\n",
    "    hist_df['curve_date'] = pd.to_datetime(hist_df['curve_date'])\n",
    "    hist_pivot = hist_df.pivot(\n",
    "        index='curve_date', columns='tenor_num', values='rate'\n",
    "    )\n",
    "\n",
    "    print('# 2) BULK FETCH CONES')\n",
    "    cones_q = f\"\"\"\n",
    "      SELECT curve_date, model_type, days_forward, tenor_num, tenor_str, cone_type, rate\n",
    "        FROM rate_cones\n",
    "       WHERE curve_type = '{CURVE_TYPE}'\n",
    "         AND curve_date BETWEEN '{min_asof}' AND '{max_asof}'\n",
    "         AND tenor_num IN ({tstr})\n",
    "    \"\"\"\n",
    "    cones_raw = ds.query(cones_q).to_pandas()\n",
    "    cones_raw['curve_date'] = pd.to_datetime(cones_raw['curve_date'])\n",
    "    cones_df = cones_raw.pivot_table(\n",
    "        index=['curve_date','model_type','days_forward','tenor_num','tenor_str'],\n",
    "        columns='cone_type', values='rate'\n",
    "    ).reset_index()\n",
    "\n",
    "    print('# 3) BULK FETCH REALIZED RATES + FORWARD-FILL')\n",
    "    cones_df['realized_date'] = cones_df['curve_date'] + pd.to_timedelta(cones_df['days_forward'], unit='d')\n",
    "    real_dates = cones_df['realized_date'].drop_duplicates()\n",
    "    dates_sql  = \",\".join(f\"'{d}'\" for d in real_dates)\n",
    "    real_q = f\"\"\"\n",
    "      SELECT curve_date AS realized_date, tenor_num, rate\n",
    "        FROM rate_curves\n",
    "       WHERE curve_type = '{CURVE_TYPE}'\n",
    "         AND curve_date IN ({dates_sql})\n",
    "         AND tenor_num IN ({tstr})\n",
    "    \"\"\"\n",
    "    real_df = ds.query(real_q).to_pandas()\n",
    "    real_df['realized_date'] = pd.to_datetime(real_df['realized_date'])\n",
    "\n",
    "    # build complete grid and forward-fill missing\n",
    "    idx = pd.MultiIndex.from_product(\n",
    "        [real_dates, TENORS], names=['realized_date','tenor_num']\n",
    "    )\n",
    "    real_complete = real_df.set_index(['realized_date','tenor_num'])['rate'].reindex(idx)\n",
    "    real_filled = real_complete.groupby(level='tenor_num').ffill().reset_index(name='rate')\n",
    "    real_map = {\n",
    "        (row['realized_date'], row['tenor_num']): row['rate']\n",
    "        for _, row in real_filled.iterrows()\n",
    "    }\n",
    "\n",
    "    print('# 4) COMPUTE & ASSEMBLE ROWS')\n",
    "    rows = []\n",
    "    for _, r in cones_df.iterrows():\n",
    "        asof  = r['curve_date']\n",
    "        start = max(\n",
    "            asof - relativedelta(years=FIT_YEARS),\n",
    "            pd.Timestamp('2010-01-01')\n",
    "        )\n",
    "        hist_slice = hist_pivot.loc[start:asof]\n",
    "\n",
    "        cov_mat   = hist_slice.cov().values\n",
    "        n_obs     = len(hist_slice)\n",
    "        total_var = float(cov_mat.sum())\n",
    "        trace_var = float(np.trace(cov_mat))\n",
    "\n",
    "        pct = { pct: r.get(f'{pct}%') for pct in [1,5,10,50,90,95,99] }\n",
    "        f50   = pct[50]\n",
    "        dfwd  = int(r['days_forward'])\n",
    "        tnum  = r['tenor_num']\n",
    "        real  = real_map.get((r['realized_date'], tnum))\n",
    "        err   = (f50 - real) if real is not None else None\n",
    "\n",
    "        rows.append((\n",
    "            CURVE_TYPE, r['model_type'], asof, dfwd, tnum, r['tenor_str'],\n",
    "            pct[1], pct[5], pct[10], pct[50], pct[90], pct[95], pct[99],\n",
    "            r['realized_date'], real,\n",
    "            err,\n",
    "            abs(err) if err is not None else None,\n",
    "            (err/real) if (err is not None and real) else None,\n",
    "            percentile_rank(real, pct) if real is not None else None,\n",
    "            bool(real is not None and pct[5] <= real <= pct[95]),\n",
    "            n_obs, total_var, trace_var\n",
    "        ))\n",
    "\n",
    "    print('# 5) BATCH UPSERT')\n",
    "    set_clause = \",\\n    \".join(f\"{c} = EXCLUDED.{c}\" for c in UPSERT_COLS)\n",
    "    for i in range(0, len(rows), BATCH_SIZE):\n",
    "        batch = rows[i:i+BATCH_SIZE]\n",
    "        vals_list = []\n",
    "        for row in batch:\n",
    "            cells = []\n",
    "            for v in row:\n",
    "                if pd.isna(v):\n",
    "                    cells.append(\"NULL\")\n",
    "                elif isinstance(v, (str, date, pd.Timestamp)):\n",
    "                    cells.append(f\"'{v}'\")\n",
    "                elif isinstance(v, bool):\n",
    "                    cells.append(\"TRUE\" if v else \"FALSE\")\n",
    "                else:\n",
    "                    cells.append(str(v))\n",
    "            vals_list.append(f\"({','.join(cells)})\")\n",
    "        vals = \",\\n\".join(vals_list)\n",
    "\n",
    "        sql = f\"\"\"\n",
    "            INSERT INTO rate_cone_diagnostics ({','.join(COLUMNS)})\n",
    "            VALUES\n",
    "            {vals}\n",
    "            ON CONFLICT (model_type, curve_date, days_forward, tenor_num) DO UPDATE\n",
    "            SET\n",
    "            {set_clause}\n",
    "            \"\"\"\n",
    "        ds.query(sql)\n",
    "\n",
    "    return rows\n",
    "\n",
    "# ─── ENTRYPOINT ──────────────────────────────────────────────────────────────\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    days = BACKDATE_DAYS\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_param(\"backdate_days\", days)\n",
    "        rows  = populate(days)\n",
    "        count = len(rows)\n",
    "        mlflow.log_metric(\"rows_upserted\", count)\n",
    "\n",
    "        # artifact\n",
    "        df = pd.DataFrame(rows, columns=COLUMNS)\n",
    "        fp = get_artifact_path(\"rate_cone_diagnostics_backfill.csv\")\n",
    "        df.to_csv(fp, index=False)\n",
    "        mlflow.log_artifact(fp, artifact_path=\"rate_cone_diagnostics\")\n",
    "\n",
    "    print(f\"✅ Done: upserted {count} rows into rate_cone_diagnostics.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518b6f1b-b19d-4cba-a71e-8ee3b1187ec0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
