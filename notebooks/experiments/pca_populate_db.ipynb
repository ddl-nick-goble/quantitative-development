{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7a2c534-c854-4afa-8d8b-f2d0996485be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting data source for sandbox\n",
      "Loading data from 2022-06-05 to 2025-06-10 (oneâ€time)...\n",
      "â†’ Running PCA for 2025-06-10...\n",
      "ğŸƒ View run PCA_2025-06-10 at: http://127.0.0.1:8768/#/experiments/1503/runs/7cf4a991565247e2847b82afe66a8c07\n",
      "ğŸ§ª View experiment at: http://127.0.0.1:8768/#/experiments/1503\n",
      "â†’ Running PCA for 2025-06-09...\n",
      "ğŸƒ View run PCA_2025-06-09 at: http://127.0.0.1:8768/#/experiments/1503/runs/fba4f2886631416bbdc3420ab5365560\n",
      "ğŸ§ª View experiment at: http://127.0.0.1:8768/#/experiments/1503\n",
      "â†’ Running PCA for 2025-06-08...\n",
      "ğŸƒ View run PCA_2025-06-08 at: http://127.0.0.1:8768/#/experiments/1503/runs/c005ed2dc66a4feda0a8fac84ad9cad4\n",
      "ğŸ§ª View experiment at: http://127.0.0.1:8768/#/experiments/1503\n",
      "â†’ Running PCA for 2025-06-07...\n",
      "ğŸƒ View run PCA_2025-06-07 at: http://127.0.0.1:8768/#/experiments/1503/runs/776eaef1004b46658b3f852b46f041f0\n",
      "ğŸ§ª View experiment at: http://127.0.0.1:8768/#/experiments/1503\n",
      "â†’ Running PCA for 2025-06-06...\n",
      "ğŸƒ View run PCA_2025-06-06 at: http://127.0.0.1:8768/#/experiments/1503/runs/8ff5aef338124835b49880eb206d38b7\n",
      "ğŸ§ª View experiment at: http://127.0.0.1:8768/#/experiments/1503\n",
      "ğŸƒ View run Rolling PCA at: http://127.0.0.1:8768/#/experiments/1503/runs/a65d06ef1f6049959dc71daf2df932da\n",
      "ğŸ§ª View experiment at: http://127.0.0.1:8768/#/experiments/1503\n",
      "âœ… All PCA runs complete.\n"
     ]
    }
   ],
   "source": [
    "import data.data_source as data_source\n",
    "import sys\n",
    "import time\n",
    "import uuid\n",
    "import json\n",
    "from datetime import date\n",
    "from functools import lru_cache\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import os\n",
    "from config import env\n",
    "from utils.artifact_saver import get_artifact_path\n",
    "\n",
    "from models.pca_model import legacy_pca, sklearn_pca\n",
    "\n",
    "# â”€â”€â”€ CONFIGURATION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "TENORS = [0.25, 0.5, 1, 2, 3, 5, 7, 10, 20, 30]\n",
    "ROLLING_YEARS = 3\n",
    "N_COMPONENTS = 3\n",
    "CURVE_TYPE = \"US Treasury Par\"\n",
    "pca_model = legacy_pca\n",
    "\n",
    "# MLflow experiment\n",
    "experiment_name = f\"PCA Training3[{env}]\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# â”€â”€â”€ DATASOURCE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ds = data_source.get_data_source()\n",
    "\n",
    "# â”€â”€â”€ ONEâ€TIME LOAD & PIVOT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def load_and_pivot_all(earliest_date: date, latest_date: date) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Pull every curve row between earliest_date and latest_date once,\n",
    "    pivot to a DateÃ—Tenor matrix, then forward/backfill missing values\n",
    "    across the entire range. Return a pivoted DataFrame with tenor columns.\n",
    "    \"\"\"\n",
    "    sql = f\"\"\"\n",
    "        SELECT curve_date, tenor_num AS tenor, rate\n",
    "          FROM rate_curves\n",
    "         WHERE curve_date BETWEEN '{earliest_date}'::date AND '{latest_date}'::date\n",
    "           AND curve_type = '{CURVE_TYPE}'\n",
    "        ORDER BY curve_date\n",
    "    \"\"\"\n",
    "    df_all = ds.query(sql).to_pandas()\n",
    "    df_all[\"curve_date\"] = pd.to_datetime(df_all[\"curve_date\"])\n",
    "    # pivot once\n",
    "    pivot = df_all.pivot(index=\"curve_date\", columns=\"tenor\", values=\"rate\")\n",
    "    # ensure all TENORS are present\n",
    "    pivot = pivot.reindex(columns=TENORS)\n",
    "    # forwardâ€fill & backâ€fill entire matrix\n",
    "    pivot_filled = pivot.ffill().bfill()\n",
    "    return pivot_filled\n",
    "\n",
    "# â”€â”€â”€ PCAâ€ANDâ€LOG FOR A SLICE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def run_pca_and_log_slice(as_of_date: date, pivot_filled: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Perform PCA on the slice of pivot_filled from (as_of_date - 3y) to as_of_date.\n",
    "    Insert the results into the DB and log metrics/artifacts to MLflow.\n",
    "    \"\"\"\n",
    "    start_date = as_of_date - relativedelta(years=ROLLING_YEARS)\n",
    "    end_date = as_of_date\n",
    "\n",
    "    # Extract the subâ€matrix for this date range:\n",
    "    # because we've already forward/backfilled, this slice has no NaNs.\n",
    "    slice_df = pivot_filled.loc[start_date:end_date]\n",
    "    X = slice_df.to_numpy()\n",
    "    num_obs, num_tenors = X.shape\n",
    "    means = X.mean(axis=0)\n",
    "    total_var = ((X - means) ** 2).mean()\n",
    "\n",
    "    # Fit PCA. If using sklearn, you'd do something like:\n",
    "    # sklearn_pca = SklearnPCA(n_components=N_COMPONENTS)\n",
    "    # all_scores = sklearn_pca.fit_transform(X)\n",
    "    # components = sklearn_pca.components_\n",
    "    # explained_ratio = sklearn_pca.explained_variance_ratio_\n",
    "    #\n",
    "    # If you stick with legacy_pca, assume it returns (components, explained_ratio, mean_curve, all_scores).\n",
    "    components, explained_ratio, mean_curve, all_scores, raw_model = pca_model(X, N_COMPONENTS)\n",
    "    today_scores = all_scores[-1]  # last row corresponds to as_of_date\n",
    "\n",
    "    # Compute reconstruction error & RÂ²\n",
    "    X_recon = all_scores @ components + mean_curve\n",
    "    mse = ((X - X_recon) ** 2).mean()\n",
    "    r2 = 1 - mse / total_var\n",
    "\n",
    "    total_explained = float(explained_ratio.sum())\n",
    "\n",
    "    # INSERT/UPSERT into DB\n",
    "    run_id = str(uuid.uuid4())\n",
    "    insert_sql = f\"\"\"\n",
    "    INSERT INTO pca_results (\n",
    "      run_id, curve_type, curve_date, n_components,\n",
    "      total_explained_variance_ratio, explained_variance_ratios,\n",
    "      mean_curve, components, scores\n",
    "    ) VALUES (\n",
    "      '{run_id}', '{CURVE_TYPE}', '{as_of_date}',\n",
    "      {N_COMPONENTS}, {total_explained},\n",
    "      ARRAY{explained_ratio.tolist()},\n",
    "      ARRAY{mean_curve.tolist()},\n",
    "      '{json.dumps(components.tolist()).replace(\"'\", \"''\")}',\n",
    "      ARRAY{today_scores.tolist()}\n",
    "    )\n",
    "    ON CONFLICT (curve_type, curve_date)\n",
    "    DO UPDATE SET\n",
    "      run_id                        = EXCLUDED.run_id,\n",
    "      run_timestamp                 = CLOCK_TIMESTAMP(),\n",
    "      n_components                  = EXCLUDED.n_components,\n",
    "      total_explained_variance_ratio= EXCLUDED.total_explained_variance_ratio,\n",
    "      explained_variance_ratios     = EXCLUDED.explained_variance_ratios,\n",
    "      mean_curve                    = EXCLUDED.mean_curve,\n",
    "      components                    = EXCLUDED.components,\n",
    "      scores                        = EXCLUDED.scores;\n",
    "    \"\"\"\n",
    "    ds.query(insert_sql)\n",
    "\n",
    "    # MLflow logging for this slice\n",
    "    mlflow.log_param(\"as_of_date\", as_of_date)\n",
    "    mlflow.log_param(\"curve_type\", CURVE_TYPE)\n",
    "    mlflow.log_param(\"n_components\", N_COMPONENTS)\n",
    "    mlflow.log_param(\"days_requested\", 1)\n",
    "    mlflow.log_param(\"rolling_years\", ROLLING_YEARS)\n",
    "    mlflow.log_param(\"pca_model_name\", pca_model.__name__)\n",
    "    mlflow.log_param(\"starting_domino_user\", os.environ.get(\"DOMINO_STARTING_USERNAME\", \"\"))\n",
    "    mlflow.log_metric(\"num_observations\", num_obs)\n",
    "\n",
    "    mlflow.log_metric(\"reconstruction_mse\", float(mse))\n",
    "    mlflow.log_metric(\"total_explained_variance\", total_explained)\n",
    "    for i, ratio in enumerate(explained_ratio, start=1):\n",
    "        mlflow.log_metric(f\"explained_variance_ratio_{i}\", float(ratio))\n",
    "    mlflow.log_metric(\"run_duration_seconds\", time.time() - mlflow_start_time_per_slice[0])\n",
    "\n",
    "    return explained_ratio  # return this so we can build the scree plot later\n",
    "\n",
    "# â”€â”€â”€ POPULATE LOOP (ONEâ€TIME LOAD + SLICE) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def populate(days: int, as_of: date):\n",
    "    \"\"\"\n",
    "    Instead of calling load_curve_data 1Ã—/day, we:\n",
    "    1) Compute the earliest date weâ€™ll need (3 years + days back).  \n",
    "    2) Pull everything once, pivot & fill.  \n",
    "    3) Loop over each as_of_date slice, run PCA & log.  \n",
    "    4) After the loop, build a consolidated screeâ€plot or CSV if desired.\n",
    "    \"\"\"\n",
    "    end_date = date.today()\n",
    "    earliest_possible = as_of - relativedelta(years=ROLLING_YEARS) - relativedelta(days=days)\n",
    "    min_date = date(2010, 3, 15)\n",
    "    if earliest_possible < min_date:\n",
    "        earliest_possible = min_date\n",
    "\n",
    "    # 1) ONEâ€TIME: load & pivot entire range\n",
    "    print(f\"Loading data from {earliest_possible} to {end_date} (oneâ€time)...\")\n",
    "    pivot_filled = load_and_pivot_all(earliest_possible, end_date)\n",
    "\n",
    "    # 2) Start MLflow parent run\n",
    "    with mlflow.start_run(run_name=\"Rolling PCA\", nested=False):\n",
    "        mlflow.log_param(\"days_requested\", days)\n",
    "        mlflow.log_param(\"rolling_years\", ROLLING_YEARS)\n",
    "        mlflow.log_param(\"n_components\", N_COMPONENTS)\n",
    "        mlflow.log_param(\"curve_type\", CURVE_TYPE)\n",
    "        mlflow.log_param(\"pca_model_name\", pca_model.__name__)\n",
    "        mlflow.log_param(\"starting_domino_user\", os.environ.get(\"DOMINO_STARTING_USERNAME\", \"\"))\n",
    "\n",
    "        # We'll collect all explained_variance_ratios to make one scree plot at the end\n",
    "        scree_data = []\n",
    "\n",
    "        # 3) Loop over each day\n",
    "        for i in range(days):\n",
    "            as_of_date = as_of - relativedelta(days=i)\n",
    "            print(f\"â†’ Running PCA for {as_of_date}...\")\n",
    "\n",
    "            # Start a nested run for this date\n",
    "            with mlflow.start_run(nested=True, run_name=f\"PCA_{as_of_date}\") as nested_run:\n",
    "                global mlflow_start_time_per_slice\n",
    "                mlflow_start_time_per_slice = (time.time(),)  # just to measure perâ€slice latency\n",
    "                explained_ratio = run_pca_and_log_slice(as_of_date, pivot_filled)\n",
    "                scree_data.append((as_of_date, explained_ratio))\n",
    "                mlflow.end_run()\n",
    "\n",
    "        # 4) After all slices are done, optionally write a combined screeâ€plot & CSV once:\n",
    "        #    This avoids ğ file writes â‡’ only 1 final write.\n",
    "        all_components = pd.DataFrame(\n",
    "            {\n",
    "                \"as_of_date\": [d for d, ratios in scree_data],\n",
    "                **{\n",
    "                    f\"pc{i+1}_ratio\": [ratios[i] for d, ratios in scree_data]\n",
    "                    for i in range(N_COMPONENTS)\n",
    "                },\n",
    "            }\n",
    "        )\n",
    "        # Save once:\n",
    "        csv_path = get_artifact_path(\"all_scree_data.csv\")\n",
    "        all_components.to_csv(csv_path, index=False)\n",
    "        mlflow.log_artifact(csv_path, artifact_path=\"pca_metrics\")\n",
    "\n",
    "        # And make one combined screeâ€plot (chains of markers per date)\n",
    "        fig, ax = plt.subplots(figsize=(8, 5))\n",
    "        for as_of_date, ratios in scree_data:\n",
    "            ax.plot(\n",
    "                np.arange(1, N_COMPONENTS + 1),\n",
    "                ratios,\n",
    "                marker=\"o\",\n",
    "                linestyle=\"-\",\n",
    "                label=str(as_of_date),\n",
    "            )\n",
    "        ax.set_xlabel(\"Principal Component\")\n",
    "        ax.set_ylabel(\"Explained Variance Ratio\")\n",
    "        ax.set_title(f\"Scree Plot Over Time (last {days} days)\")\n",
    "        ax.legend(fontsize=\"small\", ncol=2, loc=\"upper right\", bbox_to_anchor=(1.2, 1.0))\n",
    "\n",
    "        plot_path = get_artifact_path(\"all_scree_over_time.png\")\n",
    "        fig.savefig(plot_path, bbox_inches=\"tight\")\n",
    "        plt.close(fig)\n",
    "        mlflow.log_artifact(plot_path, artifact_path=\"scree_plots\")\n",
    "\n",
    "        # Aggregate metrics from child runs\n",
    "        all_ratios = np.array([r for _, r in scree_data])\n",
    "        avg_ratios = all_ratios.mean(axis=0)\n",
    "\n",
    "        mlflow.log_metric(\"explained_variance_ratio_1\", float(avg_ratios[0]))\n",
    "        mlflow.log_metric(\"explained_variance_ratio_2\", float(avg_ratios[1]))\n",
    "        mlflow.log_metric(\"explained_variance_ratio_3\", float(avg_ratios[2]))\n",
    "        mlflow.log_metric(\"total_explained_variance\", float(avg_ratios.sum()))\n",
    "\n",
    "        # Get run metadata from nested runs\n",
    "        from mlflow.tracking import MlflowClient\n",
    "        client = MlflowClient()\n",
    "        child_runs = client.search_runs([mlflow.active_run().info.experiment_id],\n",
    "                                        f\"tags.mlflow.parentRunId = '{mlflow.active_run().info.run_id}'\")\n",
    "\n",
    "        mse_vals = [float(r.data.metrics.get(\"reconstruction_mse\", 0)) for r in child_runs]\n",
    "        duration_vals = [float(r.data.metrics.get(\"run_duration_seconds\", 0)) for r in child_runs]\n",
    "        obs_vals = [int(r.data.metrics.get(\"num_observations\", 0)) for r in child_runs]\n",
    "\n",
    "        mlflow.log_metric(\"reconstruction_mse\", float(np.mean(mse_vals)))\n",
    "        mlflow.log_metric(\"run_duration_seconds\", float(np.sum(duration_vals)))\n",
    "        mlflow.log_metric(\"num_observations\", int(np.mean(obs_vals)))\n",
    "        mlflow.log_param(\"as_of_date\", str(max([d for d, _ in scree_data])))\n",
    "\n",
    "    print(\"âœ… All PCA runs complete.\")\n",
    "\n",
    "# â”€â”€â”€ MAIN â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if __name__ == \"__main__\":\n",
    "    default_backdated_days = 5\n",
    "    default_as_of = date.today()\n",
    "\n",
    "    if len(sys.argv) > 1:\n",
    "        try:\n",
    "            as_of = date.fromisoformat(sys.argv[1])\n",
    "        except ValueError:\n",
    "            as_of = default_as_of\n",
    "    else:\n",
    "        as_of = default_as_of\n",
    "\n",
    "    populate(default_backdated_days, as_of)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138a932d-9e3c-41c9-8458-8936918a14c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
